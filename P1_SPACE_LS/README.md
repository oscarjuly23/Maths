# Space LS: Aprenentatge autom√†tic i les xarxes neuronals.

## Introducci√≥n
La Inteligencia Artificial (IA) es una rama de la inform√°tica dedicada a la creaci√≥n de m√°quinas inteligentes capaces de realizar tareas que normalmente requieren inteligencia humana. En esta pr√°ctica, nos centraremos en el Aprendizaje Autom√°tico (Machine Learning) y, m√°s espec√≠ficamente, en las redes neuronales, que son fundamentales en el campo del Aprendizaje Profundo o Deep Learning.

El Aprendizaje Autom√°tico, como su nombre indica, es un subcampo de la IA que se basa en algoritmos y aplicaciones que no requieren programaci√≥n expl√≠cita, sino que aprenden de la experiencia.

## Reto
La empresa SpaceLS, propiedad del multimillonario Eloi Mas, ha lanzado miles de sat√©lites al espacio. Desafortunadamente, un error de programaci√≥n ha provocado que algunos de estos sat√©lites pierdan su rumbo y se muevan sin control a una altitud de 1100 km sobre la Tierra.

Eloi Mas ha pedido ayuda a los estudiantes de la asignatura de Estad√≠stica y An√°lisis Matem√°tico para poder rescatar los sat√©lites. Los ingenieros de SpaceLS han realizado dos descubrimientos clave:

1. Hay dos tipos de sat√©lites: los desprogramados (llamados "clase 0") y los no desprogramados (llamados "clase 1").
2. A partir de las coordenadas (ùë•ùë•,ùë¶ùë¶) de cada sat√©lite en el espacio, se puede determinar si un sat√©lite pertenece a la clase 0 o la clase 1.
El objetivo es dise√±ar e implementar una red neuronal capaz de clasificar los sat√©lites en clase 0 o clase 1 seg√∫n sus coordenadas.

## Teor√≠a

### Qu√© es un Clasificador
Un clasificador es un algoritmo que asigna etiquetas a objetos. Para hacer esta asignaci√≥n, el clasificador "mapea" los objetos a etiquetas pertenecientes a un conjunto predefinido de categor√≠as.

### Entrenar y Probar un Clasificador
Para entrenar un clasificador, se necesitan datos etiquetados. Estos datos se dividen en conjuntos de entrenamiento y prueba. Los conjuntos de entrenamiento se utilizan para ense√±ar al clasificador, mientras que los de prueba se utilizan para evaluar su rendimiento.

### Los Datos
En esta pr√°ctica, los datos no son im√°genes, sino coordenadas (ùë•ùë•,ùë¶ùë¶) de sat√©lites de SpaceLS, que se dividen en dos clases: clase 0 y clase 1.

### Clasificadores Liniales
Los clasificadores pueden tomar muchas formas, y en esta pr√°ctica implementar√°s un clasificador lineal en forma de red neuronal.

###  √àpoques
Cuando entrenas un clasificador, el proceso de ajuste de par√°metros se realiza en √©pocas. Cada √©poca pasa todos los datos de entrenamiento por el clasificador.

###  Funci√≥ de p√®rdues
La funci√≥n de p√©rdida cuantifica cu√°n buena o mala es la predicci√≥n de un clasificador. El objetivo es minimizar esta p√©rdida ajustando los par√°metros del clasificador.

###  Optimitzaci√≥
La optimizaci√≥n implica ajustar iterativamente los par√°metros del clasificador para minimizar la funci√≥n de p√©rdida. El algoritmo de optimizaci√≥n m√°s com√∫n es el descenso de gradiente.

###  Algorismes d'optimitzaci√≥
Implementar√°s dos variantes del algoritmo de descenso de gradiente: el descenso de gradiente cl√°sico y el descenso de gradiente estoc√°stico.

## Experiments
###  Influ√®ncia del valor de Œ±
En este experimento, evaluamos c√≥mo el par√°metro Œ±, que controla la velocidad de convergencia y la precisi√≥n en el algoritmo de descenso de gradiente estoc√°stico (SGD), afecta el rendimiento. Realizamos entrenamientos con diferentes valores de Œ± (0.1, 0.01, 0.001 y 0.0001) en el conjunto de datos data1.mat durante 10 √©pocas. Observamos las siguientes conclusiones:

- Valores peque√±os de Œ± (como 0.01 y 0.001) conducen a una convergencia m√°s r√°pida.
- Un valor de Œ± inadecuadamente grande (0.1) puede hacer que el algoritmo diverja.
- Elegir el valor √≥ptimo de Œ± es fundamental para el √©xito de SGD.

###   GD vs SGD
En este experimento, comparamos el rendimiento del algoritmo de descenso de gradiente (GD) y el descenso de gradiente estoc√°stico (SGD) en el conjunto de datos data2.mat. Ambos se entrenaron durante 100 √©pocas con un valor de Œ± de 0.001 y un tama√±o de lote de 5 para SGD. Estas son nuestras observaciones:

- El GD converge m√°s r√°pido que el SGD en este conjunto de datos.
- El SGD tiene un rendimiento intermedio, ya que el conjunto de datos no es linealmente separable.
- La elecci√≥n del tama√±o del lote (batch size) es crucial para el rendimiento del SGD.

###  Influ√®ncia de la mida del batch en SGD
Este experimento se centr√≥ en estudiar c√≥mo el tama√±o del lote afecta el rendimiento del algoritmo de descenso de gradiente estoc√°stico (SGD) en el conjunto de datos data4.mat. Se realizaron entrenamientos durante 100 √©pocas con Œ± = 0.001 y diferentes tama√±os de lote (5, 10, 20 y 50). Las conclusiones son las siguientes:

- Un tama√±o de lote m√°s peque√±o tiende a minimizar la funci√≥n de p√©rdida de manera m√°s efectiva.
- Aumentar el tama√±o del lote no necesariamente mejora la convergencia debido a la naturaleza no linealmente separable del conjunto de datos.

###  Comparativa de la taxa d‚Äôencert en els datasets
En este experimento, aplicamos el algoritmo de descenso de gradiente estoc√°stico (SGD) a cuatro conjuntos de datos diferentes (data1.mat, data2.mat, data3.mat y data4.mat) durante 100 √©pocas, con Œ± = 0.001 y un tama√±o de lote de 5. Estos son los resultados de la tasa de acierto (accuracy):

- data1.mat: Accuracy del 100% debido a la buena separaci√≥n lineal de los datos.
- data2.mat: Accuracy del 50% debido a la falta de separaci√≥n lineal en los datos.
- data3.mat: El accuracy es el m√°s bajo, lo que sugiere que este conjunto de datos es especialmente desafiante para SGD.
- data4.mat: Accuracy del 85.4% a pesar de la dificultad de separaci√≥n lineal, lo que indica un buen rendimiento.

Estos experimentos ilustran c√≥mo diferentes par√°metros y conjuntos de datos pueden influir en el rendimiento de SGD y resaltan la importancia de ajustar adecuadamente los hiperpar√°metros para cada problema.
